{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style= \"background-color: #f8f9fa; color: #333; padding: 15px;\">\n",
    "    <a class=\"anchor\" id=\"1st-bullet\"></a>\n",
    "    <h3 style=\"color: #198754;font-weight: bold;\">DATA MINING GROUP PROJECT</h3>\n",
    "    <h2 style=\"color: #198754; font-weight: bold;\">ABCDEats Inc. Final Report</h3>\n",
    "    <h3 style=\"color: #198754;\">Group 20</h3>\n",
    "    Afonso Gamito, 20240752 <br>\n",
    "    Gonçalo Pacheco, 20240695<br>\n",
    "    Hassan Bhatti, 20241023<br>\n",
    "    Moeko Mitani, 20240670 <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Index\n",
    "\n",
    "- [Importing the Libraries and CVS file](#1) \n",
    "- [1. Checking The Dataset](#2)\n",
    "    - [1.1. Metadata](#3)\n",
    "    - [1.2. Issues in The Dataset](#4)\n",
    "- [2. Data Visualization](#5)\n",
    "- [3. Key Statistics and Trends](#6)\n",
    "- [4. Data Pre-Processing 1](#7)\n",
    "    - [4.1. Treating Missing Values](#8)\n",
    "    - [4.2. Changing Data Types](#9)\n",
    "    - [4.3. Dropping Duplicates](#10)\n",
    "    - [4.4. Dropping Customers with No Order](#11)\n",
    "- [5. Outliers](#12) \n",
    "- [6. New Features Creation](#13)\n",
    "    - [6.1. days_as_customer](#14)\n",
    "    - [6.2. money_spent](#15)\n",
    "    - [6.3. customer_city](#16)\n",
    "    - [6.4. total_orders](#17)\n",
    "    - [6.5. average_purchase](#18)\n",
    "    - [6.6. age_group](#19)\n",
    "    - [6.7. morning_orders, afternoon_orders, evening_orders, night_orders](#20)\n",
    "    - [6.8. customer_frequency](#21)\n",
    "    - [6.9. customer_preference](#22)\n",
    "    - [6.10. order_frequency](#23)\n",
    "    - [6.11. weekend_orders](#24)\n",
    "    - [6.12. week_orders](#25)\n",
    "    - [6.13. Display New Features](#26)\n",
    "- [7. Data Pre-Processing 2](#27)\n",
    "    - [7.1. Checking The Dataset](#28)\n",
    "    - [7.2. Visualization of New Features](#29)\n",
    "    - [7.3. Treating Outliers of New Features ](#30)\n",
    "- [8. Feature Engineering](#31)\n",
    "    - [8.1. Dropping features](#32)\n",
    "    - [8.2. Changing Data Types](#33)\n",
    "    - [8.3. Splitting Metric Features and Non-Metric Features](#34)\n",
    "- [9. Data Scaling](#35)\n",
    "- [10. Checking Redundancy and Relevency](#36)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"1\">     \n",
    "\n",
    "# Importing the Libraries and CVS file\n",
    "\n",
    "</a>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Libraries\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "\n",
    "# Visualization \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CVS file\n",
    "file_path = 'project_data/DM2425_ABCDEats_DATASET.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"2\">     \n",
    "\n",
    "# 1. Checking The Dataset\n",
    "</a>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"3\">     \n",
    "\n",
    "## 1.1. Metadata\n",
    "</a>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains the customer data from ABCDEats Inc. collected over three months from three cities.\n",
    "\n",
    "| Feature | Feature description |\n",
    "|---------|---------------------|\n",
    "| *customer_id* | Unique identifier for each customer.|\n",
    "| *customer_region* | Geographic region where the customer is located.|\n",
    "| *customer_age* | Age of the customer.|\n",
    "| *vendor_count* | Number of unique vendors the customer has ordered from.|\n",
    "| *product_count* | Total number of products the customer has ordered.|\n",
    "| *is_chain* | Indicates whether the customer’s order was from a chain restaurant.|\n",
    "| *first_order* | Number of days from the start of the dataset when the customer first placed an order.|\n",
    "| *last_order* | Number of days from the start of the dataset when the customer most recently placed an order.|\n",
    "| *last_promo* | The category of the promotion or discount most recently used by the customer.|\n",
    "| *payment_method* | Method most recently used by the customer to pay for their orders.|\n",
    "| *CUI_American* | The amount in monetary units spent by the customer from the American cuisine.|\n",
    "| *CUI_Asian* | The amount in monetary units spent by the customer from the Asian cuisine.|\n",
    "|*CUI_Beverages* | The amount in monetary units spent by the customer from the Beverages|\n",
    "| *CUI_Cafe* | The amount in monetary units spent by the customer from the Cafe.|\n",
    "| *CUI_Chicken Dishes* | The amount in monetary units spent by the customer from the Chicken Dishes.|\n",
    "| *CUI_Chinese* | The amount in monetary units spent by the customer from the Chinese cuisine.|\n",
    "| *CUI_Desserts* | The amount in monetary units spent by the customer from the Desserts.|\n",
    "| *CUI_Healthy* | The amount in monetary units spent by the customer from the Healthy cuisine.|\n",
    "| *CUI_Indian* | The amount in monetary units spent by the customer from the Indian cuisine.|\n",
    "| *CUI_Italian* | The amount in monetary units spent by the customer from the Italian cuisine.|\n",
    "| *CUI_Japanese* | The amount in monetary units spent by the customer from the Japanese cuisine.|\n",
    "| *CUI_Noodle Dishes* | The amount in monetary units spent by the customer from the Noodle Dishes.|\n",
    "| *CUI_OTHER* | The amount in monetary units spent by the customer from the Other cuisine.|\n",
    "| *CUI_Street Food / Snacks* | The amount in monetary units spent by the customer from the Street Food / Snacks.|\n",
    "| *CUI_Thai* | The amount in monetary units spent by the customer from the Thai cuisine.|\n",
    "| *DOW_0* | Number of orders placed on each day of Sunday.|\n",
    "| *DOW_1* | Number of orders placed on each day of Monday.|\n",
    "| *DOW_2* | Number of orders placed on each day of Tuesday.|\n",
    "| *DOW_3* | Number of orders placed on each day of Wednesday.|\n",
    "| *DOW_4* | Number of orders placed on each day of Thursday.|\n",
    "| *DOW_5* | Number of orders placed on each day of Friday.|\n",
    "| *DOW_6* | Number of orders placed on each day of Saturday.|\n",
    "| *HR_0* | Number of orders placed during 12AM.|\n",
    "| *HR_1* | Number of orders placed during 1AM.|\n",
    "| *HR_2* | Number of orders placed during 2AM.|\n",
    "| *HR_3* | Number of orders placed during 3AM.|\n",
    "| *HR_4* | Number of orders placed during 4AM.|\n",
    "| *HR_5* | Number of orders placed during 5AM.|\n",
    "| *HR_6* | Number of orders placed during 6AM.|\n",
    "| *HR_7* | Number of orders placed during 7AM.|\n",
    "| *HR_8* | Number of orders placed during 8AM.|\n",
    "| *HR_9* | Number of orders placed during 9AM.|\n",
    "| *HR_10* | Number of orders placed during 10AM.|\n",
    "| *HR_11* | Number of orders placed during 11AM.|\n",
    "| *HR_12* | Number of orders placed during 12PM.|\n",
    "| *HR_13* | Number of orders placed during 1PM.|\n",
    "| *HR_14* | Number of orders placed during 2PM.|\n",
    "| *HR_15* | Number of orders placed during 3PM.|\n",
    "| *HR_16* | Number of orders placed during 4PM.|\n",
    "| *HR_17* | Number of orders placed during 5PM.|\n",
    "| *HR_18* | Number of orders placed during 6PM.|\n",
    "| *HR_19* | Number of orders placed during 7PM.|\n",
    "| *HR_20* | Number of orders placed during 8PM.|\n",
    "| *HR_21* | Number of orders placed during 9PM.|\n",
    "| *HR_22* | Number of orders placed during 10PM.|\n",
    "| *HR_23* | Number of orders placed during 11PM.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 56 features and 31888 lows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"Number of Features: {df.shape[1]}\")\n",
    "print(f\"Number of Observations: {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include = \"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4\">     \n",
    "\n",
    "## 1.2. Issues in the dataset \n",
    "</a>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found the issues below in the dataset in the previous report\n",
    "\n",
    "**Missing Values**\n",
    "- ***customer_age***: 727 (NaN)\n",
    "- ***first_order***: 106 (NaN)\n",
    "- ***HR_0***: 1165 (NaN)\n",
    "- ***customer_region***: 442 (-)\n",
    "- ***last_promo***: 16748 (-)\n",
    "\n",
    "**Wrong Data Types**\n",
    "- ***customer_age*** should be int instead of float\n",
    "- ***vendor_count*** should be bool instead of int (conflict with Metadata)\n",
    "- ***first_order*** should be int instead of float\n",
    "- ***HR_0*** should be int instead of float\n",
    "\n",
    "**Duplicates**\n",
    "- 13 duplicates\n",
    "\n",
    "<br><br>\n",
    "In this report, we found the issue below\n",
    "**Customer with no orders**\n",
    "- 138 customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a class=\"anchor\" id=\"5\">     \n",
    "\n",
    "# 2. Data Visualization\n",
    "</a>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not apply changes directly to our dataset, thus we are going to make its copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms: *customer_age* and *vender_count*\n",
    "\n",
    "### Remarks:\n",
    "- The majority of food delivery customers fall within the younger demographic range, predominantly between the ages of 20s and early 30s.\n",
    "- Most of the customers have ordered from less than four vendors in three months.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_rows = 1\n",
    "sp_cols = 2\n",
    "\n",
    "fig, axes = plt.subplots(sp_rows, sp_cols, figsize=(20, 11))\n",
    "\n",
    "# Iterate over each axis and feature to plot histograms\n",
    "for ax, feature in zip(axes.flatten(), ['customer_age', 'vendor_count']):\n",
    "    ax.hist(df_copy[feature].dropna(), bins=20, color='grey', edgecolor='black')\n",
    "    ax.set_title(f'Distribution of {feature.capitalize().replace(\"_\", \" \")}', fontsize=14, y=-0.2)\n",
    "    ax.set_xlabel(feature.capitalize().replace(\"_\", \" \"))\n",
    "    ax.set_ylabel(\"Customer Number\")\n",
    "\n",
    "plt.suptitle(\"Histograms of Customer Age and Vendor Count\", fontsize=18, y=1.02)\n",
    "\n",
    "# Ensure the directory exists for saving the figure\n",
    "save_dir = os.path.join('..', 'figures', 'eda')\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "plt.savefig(os.path.join(save_dir, 'customer_age_vendor_count_histograms.png'), dpi=200, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms: **Day as Customer**\n",
    "This shows us how long the customer has stayed with our service in three months.\n",
    "\n",
    "### Remarks\n",
    "- It can be seen that most clients stay for less than 5 days.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference between \"last_order\" and \"first_order\"\n",
    "df_copy['day_as_customer'] = df_copy['last_order'] - df_copy['first_order']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(df_copy['day_as_customer'], bins=20, color='gray', edgecolor='black')\n",
    "plt.title('Histogram of Day as Customer')\n",
    "plt.xlabel('Day as client')\n",
    "plt.ylabel('Customer Number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar Chart: **Total Orders per Hour**\n",
    "\n",
    "### Remarks\n",
    "- It is observed that high demand occurs from 10:00 to 12:00 and 16:00 to 18:00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the HR_ features\n",
    "hour_columns = [f'HR_{i}' for i in range(24)]\n",
    "\n",
    "# Calculate total orders for each hour by summing across all rows for each hour column\n",
    "total_orders_per_hour = df_copy[hour_columns].sum()\n",
    "\n",
    "# Create a bar plot to visualize total orders per hour in shades of grey\n",
    "plt.figure(figsize=(10, 6))\n",
    "total_orders_per_hour.plot(kind='bar', color=\"#9391A0\", edgecolor='black')\n",
    "plt.title('Total Orders per Hour') \n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Number of Orders')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar Chart: **Total Orders per Day of the Week**\n",
    "\n",
    "### Remarks\n",
    "- There is a peak in food orders on Thursdays (DOW_4) and Saturdays (DOW_6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns for days of the week (DOW_0 to DOW_6)\n",
    "dow_columns = [f'DOW_{i}' for i in range(7)]\n",
    "\n",
    "# Calculate total orders for each day of the week by summing across all rows for each day column\n",
    "total_orders_per_dow = df_copy[dow_columns].sum()\n",
    "\n",
    "# Create a bar chart to visualize total orders per day of the week in grey scale\n",
    "plt.figure(figsize=(10, 6))\n",
    "total_orders_per_dow.plot(kind='bar', color=\"#9391A0\", edgecolor='black')\n",
    "plt.title('Total Orders Per Day of the Week', fontsize=16) \n",
    "plt.xlabel('Day of the Week', fontsize=12)\n",
    "plt.ylabel('Number of Orders', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right') \n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar chart: **Total Orders per Cuisine**\n",
    "\n",
    "### Remarks\n",
    "- Asian, American and Street food / snacks can be considered as the most popular choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select CUI_ features\n",
    "cuisine_columns = [col for col in df_copy.columns if 'CUI_' in col]\n",
    "\n",
    "# Sum total number of orders per cuisine\n",
    "df_copy['total_orders_per_cuisine'] = df_copy[cuisine_columns].sum(axis=1)\n",
    "\n",
    "# Create a bar chart to visualize total orders per cuisine in grey scale\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_copy[cuisine_columns].sum().plot(kind='bar', color=\"#9391A0\", edgecolor='black')\n",
    "plt.title('Total Orders Per Cuisine', fontsize=16) \n",
    "plt.xlabel('Cuisine Type', fontsize=12)\n",
    "plt.ylabel('Number of Orders', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right') \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar Chart: **Average Purchases per Order by Cuisine**\n",
    "### Remarks\n",
    "- Customers who ordered CUI_cafe had the highest average purchase price, followed by CUI_Street Food / Snacks and CUI_Chinese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if \"total_revenue\" exists; if not, create it by summing across cuisine columns\n",
    "if 'total_revenue' not in df_copy.columns:\n",
    "    # Calculate total revenue for each row\n",
    "    df_copy['total_revenue'] = df_copy[cuisine_columns].sum(axis=1)  \n",
    "\n",
    "# Calculate Avg_Purchases for each cuisine\n",
    "# Multiply the number of purchases by total revenue, then sum and divide by total purchases for each cuisine\n",
    "avg_purchases = (df_copy[cuisine_columns].multiply(df_copy['total_revenue'], axis=0)).sum() / df_copy[cuisine_columns].sum()\n",
    "\n",
    "# Plot Avg_Purchases as a bar chart\n",
    "plt.figure(figsize=(10, 6))  \n",
    "plt.bar(cuisine_columns, avg_purchases, color=\"#9391A0\", edgecolor='black')  \n",
    "\n",
    "plt.title('Average Purchases per Order by Cuisine', fontsize=16)  \n",
    "plt.xlabel('Cuisine Type', fontsize=12)  \n",
    "plt.ylabel('Avg Purchases ($)', fontsize=12)  \n",
    "plt.xticks(rotation=45, ha='right')  \n",
    "plt.tight_layout()  \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar Chart: **Total Orders by Customer Region**\n",
    "### Remarks\n",
    "- The most orders were placed in 8,670 regions, followed by 4,660 and 2,360 regions.\n",
    "- There should be 3 cities instead of 9 regions in the dataset. It seems like these cities are: City A where the number begins with 2, City B where the number begins with 4, City C where the number begins with 8.\n",
    "- The missing value \"-\" not a large number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total orders by summing across all cuisine columns\n",
    "df_copy['money_spent'] = df_copy[cuisine_columns].sum(axis=1)\n",
    "\n",
    "# Group data by customer region and calculate total orders for each region\n",
    "region_orders = df_copy.groupby('customer_region')['money_spent'].sum().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))  \n",
    "sns.barplot(x=region_orders.index, y=region_orders.values, color='#9391A0', edgecolor='black')  \n",
    "plt.title('Total Money Spent by Customer Region', fontsize=16)  \n",
    "plt.xlabel('Customer Region', fontsize=12)  \n",
    "plt.ylabel('Money Spent', fontsize=12)  \n",
    "plt.xticks(rotation=45) \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar Chart: **Total Orders by Customer Region and Cuisine Type**\n",
    "- In regions beginning with 8, **CUI_Asian** is the most popular, followed by CUI_Street Food / Snack and CUI_American. \n",
    "- In regions beginning with 4, **CUI_Italian** is the most popular, followed by CUI_American and CUI_Asian. \n",
    "- In regions beginning with 2, **CUI_Asian** and **American** are the most popular, followed by CUI_Italian. \n",
    "- The missing value “-” has the same distribution as the regions starting with the number “8”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summing total orders by cuisine and region of the popular cuisines \n",
    "We defined \"CUI_Asian\", \"CUI_American\", \"CUI_Italian\", and \"CUI_Street Food / Snacks\" as popular cuisines referring to total_orders_per_cuisine histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuisine_columns_more_popular = ['CUI_Asian', 'CUI_American', 'CUI_Italian', 'CUI_Street Food / Snacks']\n",
    "money_spent_by_region = df_copy.groupby('customer_region')[cuisine_columns_more_popular].sum()\n",
    "\n",
    "# Reset the index for plotting\n",
    "money_spent_by_region = money_spent_by_region.reset_index()\n",
    "\n",
    "# Melt the DataFrame to get a long-form version suitable for seaborn\n",
    "money_spent_by_region_melted = money_spent_by_region.melt(id_vars='customer_region', \n",
    "                                                          value_vars=cuisine_columns_more_popular, \n",
    "                                                          var_name='Cuisine', \n",
    "                                                          value_name='money_spent')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(data=money_spent_by_region_melted, x='customer_region', y='money_spent', hue='Cuisine', palette=\"muted\")\n",
    "plt.title('Money Spent by Region and Cuisine Type')\n",
    "plt.xlabel('Customer Region')\n",
    "plt.ylabel('Money spent')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Cuisine Type')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summing total orders by cuisine that are less popular "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuisine_columns_less_popular = ['CUI_Beverages', 'CUI_Cafe', 'CUI_Chicken Dishes', 'CUI_Chinese', 'CUI_Desserts', 'CUI_Healthy', 'CUI_Indian', 'CUI_Japanese', 'CUI_Noodle Dishes', 'CUI_OTHER', 'CUI_Thai']\n",
    "money_spent_by_region = df_copy.groupby('customer_region')[cuisine_columns_less_popular].sum()\n",
    "\n",
    "# Reset the index for plotting\n",
    "money_spent_by_region = money_spent_by_region.reset_index()\n",
    "\n",
    "# Melt the DataFrame to get a long-form version suitable for seaborn\n",
    "money_spent_by_region_melted = money_spent_by_region.melt(id_vars='customer_region', \n",
    "                                                          value_vars=cuisine_columns_less_popular, \n",
    "                                                          var_name='Cuisine', \n",
    "                                                          value_name='money_spent')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(data=money_spent_by_region_melted, x='customer_region', y='money_spent', hue='Cuisine', palette=\"muted\")\n",
    "plt.title('Money Spent by Region and Cuisine Type')\n",
    "plt.xlabel('Customer Region')\n",
    "plt.ylabel('Money spent')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Cuisine Type')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"6\">     \n",
    "\n",
    "# 3. Key Statisctics and Trend\n",
    "</a>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks from first delivery\n",
    "\n",
    "### Key Statistics\n",
    "- **Customer Age**: The mean of 'customer_age' is 27.5. 25% of the customers are 23 or younger, and 75% are 32 or younger. Thus, it can be concluded that young people tend to use our food delivery service.\n",
    "- **Vender Number**: The mean of 'vendor_count' is 3.1. 25% of the customers have ordered from 1 vendor, 50% have ordered from 2 vendors, and 75% have ordered from 4 vendors. Thus, it can be concluded that most of the customers have ordered from less than 4 vendors in 3 months.\n",
    "- **Product Number**: The mean of 'product_count' is 5.67. 25% of the customers have ordered 2 products, 50% have ordered 3 products, and 75% have ordered 7 products in 3 months. There is a high standard deviation of 6.96, reflecting significant variability in order volume.\n",
    "- **Payment Methods**: Most customers pay by card rather than by cash or digital.\n",
    "- **Order Numbers per Regions**: The region with the highest number of orders was 8670, followed by 4660 and 2360, far ahead of the other regions. The region 8670 had the highest number of unique customers who placed orders, followed by 4660 and 2360.\n",
    "\n",
    "### Key Decisions\n",
    "- **Chain Restaurant Number**: DISAGREE WITH METADATA. The values should be converted to boolean in the next process.\n",
    "- **Date of First Order and Last Order**: We decided to creat new feature .\n",
    "- **Customer Region**: There are 8 different regions. There should be 3 cities instead, so it seems like they are postal codes. Then we can categorize them into 3 groups (cities): city A (2360, 2440 and 2490), city B (4660 and 4140), and city C (8670, 8370 and 8550) in the next process.\n",
    "\n",
    "### Trends\n",
    "- **Young Customers**: The majority of food delivery customers fall within the younger demographic range, predominantly between the ages of 18 and 32.\n",
    "- **Payment by Card**: Most customers pay by card rather than by cash or digital.\n",
    "- **Popular Cuisine**: American and Asian dishes emerge as the most popular choices.\n",
    "- **Popular Cuisine per Regions**: Asian cuisine is the most popular in region 8670, while Italian cuisine leads in region 4660. This trend is consistent across other city regions, though with significant variation in total order volumes. In region 4140, however, the popularity of these cuisines is less pronounced compared to the two largest regions. City A shows greater diversity in cuisine preferences than the other two cities.\n",
    "- **Busier Day of The Week**: Peak food ordering on Thursdays (DOW_4) and Saturdays (DOW_6).\n",
    "- **Busier Hours**: High demand occurs from 10:00 AM to 12:00 PM and 4:00 PM to 6:00 PM.\n",
    "- **Customer Frequency**: Most of the customers stay for two to three days, this short engagement period suggests that customers may use the service sporadically rather than consistently over extended periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"7\">     \n",
    "\n",
    "# 4. Data Pre-Processing 1\n",
    "</a>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"8\">     \n",
    "\n",
    "## 4.1. Treating Missing Values\n",
    "</a>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks from first delivery\n",
    "[Missing values]\n",
    "- ***customer_age***: 727 (NaN)\n",
    "- ***first_order***: 106 (NaN)\n",
    "- ***HR_0***: 1165 (NaN)\n",
    "- ***customer_region***: 442 (-)\n",
    "- ***last_promo***: 16748 (-) --> <font color='red'>Exaggerated number of missing values</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the figure of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count both NaN and '-' as missing values\n",
    "missing_values = df.isnull().sum() + df.isin(['-']).sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "\n",
    "# Create a summary of missing values\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Percentage (%)': missing_percentage\n",
    "}).sort_values(by='Percentage (%)', ascending=False)\n",
    "\n",
    "print(\"\\nMissing Data Summary:\")\n",
    "missing_summary[missing_summary['Missing Values'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary copy of the dataset for visualization\n",
    "temp_df = df.copy()\n",
    "\n",
    "# Replace '-' with NaN for visualization purposes\n",
    "temp_df.replace('-', np.nan, inplace=True)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(temp_df.isnull(), cbar=False, cmap='CMRmap')\n",
    "plt.title('Missing Data Heatmap (Including \"-\" as Missing)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided to fill missing values in the feature: \n",
    "- ***customer_age***: with its median because the data is skewed.\n",
    "- ***first_order***: with its median because the data is skewed.\n",
    "- ***HR_0*** with 0 because it is the only value it has.\n",
    "- ***customer_region*** with 8000. From the visualization, \"-\" seems like to be able to be considered as City C (starting with 8000). Thus, we can fill the missing value \"-\" with \"8000\" for now since we will merge it with \"8670\", \"8370\" and \"8550\".\n",
    "- ***last_promo***: with \"Unknown\" for now. We might frop the feature itself later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMissing Values Before Cleaning:\")\n",
    "print(temp_df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check customer_region\n",
    "df[\"customer_region\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check customer_region\n",
    "df[\"customer_region\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check last_promo\n",
    "df[\"last_promo\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in customer age with its median\n",
    "df[\"customer_age\"] = df[\"customer_age\"].fillna(df[\"customer_age\"].median())\n",
    "\n",
    "# Fill missing values in first_order with its median\n",
    "df[\"first_order\"] = df[\"first_order\"].fillna(df[\"first_order\"].median())\n",
    "\n",
    "# Fill missing values in HR_0 with 0\n",
    "df[\"HR_0\"] = df[\"HR_0\"].fillna(df[\"HR_0\"].fillna(0))\n",
    "\n",
    "# Fill missing values \"-\" in customer_region with 8000\n",
    "df[\"customer_region\"] = df[\"customer_region\"].replace(\"-\", '8000')\n",
    "\n",
    "# Fill missing values \"-\" in last_prom with UNKNOWN\n",
    "df[\"last_promo\"] = df[\"last_promo\"].replace(\"-\", \"UNKNOWN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"customer_region\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMissing Values After Cleaning:\")\n",
    "print(df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"9\">     \n",
    "\n",
    "## 4.2. Changing Data Types\n",
    "</a>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks\n",
    "- ***customer_age*** should be integer instead of float.\n",
    "- ***is_chain*** should be boolean instead of integer because we agreed with METADATA.\n",
    "- ***first_order*** should be integer instead of float.\n",
    "- ***HR_0*** should be integer instead of float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"customer_age\"] = df[\"customer_age\"].astype(\"int\")\n",
    "\n",
    "df[\"first_order\"] = df[\"first_order\"].astype(\"int\")\n",
    "\n",
    "df[\"HR_0\"] = df[\"HR_0\"].astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We agree with Metadata and disagree with the dataset about *is_chain*. We decided to convert it into boolean. If the value = 0, that means False, True otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"is_chain\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"is_chain\"] = df[\"is_chain\"].astype(\"bool\")\n",
    "\n",
    "#Replace True with 1 and False with 0\n",
    "df[\"is_chain\"] = df[\"is_chain\"].apply(lambda x: True if x else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset if all dtypes are correct\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"10\">     \n",
    "\n",
    "## 4.3. Dropping Duplicates\n",
    "</a>    \n",
    "\n",
    "We are going to check if there are duplicates in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are duplicates in the dataset\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 13 duplicates. Since it is a small number, we decided to drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates in the dataset\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all duplicates are dropped\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"11\">     \n",
    "\n",
    "## 4.4. Dropping Customers with No Order\n",
    "</a>    \n",
    "\n",
    "We are going to check if there are customers who have not ordered anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to sum\n",
    "columns_to_sum = ['DOW_0', 'DOW_1', 'DOW_2', 'DOW_3', 'DOW_4', 'DOW_5', 'DOW_6']\n",
    "\n",
    "# Create new feature 'total_order' \n",
    "df['total_orders'] = df[columns_to_sum].sum(axis=1)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_with_zero_orders = df[df['total_orders'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_with_zero_orders.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found out that there are 138 customers who have not ordered anything in the dataset. Since it is not a large number, we have decided to drop them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['total_orders'] == 0].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the change was applied correctly\n",
    "df['total_orders'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'total_orders'\n",
    "df.drop('total_orders', axis=1, inplace=True)\n",
    "\n",
    "# Check if the change was applied correctly\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"12\">     \n",
    "\n",
    "# 5. Outliers Treatment\n",
    "</a>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define the outliers, we are going to create box plots of numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box plots for numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a variable of numerical features\n",
    "numeric_columns = df.select_dtypes(include=['float64', 'int32', 'int64']).columns\n",
    "\n",
    "# Plot boxplots \n",
    "def plot_boxplots_grid(df, columns, cols=3):\n",
    "    rows = int(np.ceil(len(columns) / cols))\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 4))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, column in enumerate(columns):\n",
    "        sns.boxplot(x=df[column], ax=axes[i], color='grey')\n",
    "        axes[i].set_title(f'Boxplot of {column}')\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_boxplots_grid(df, numeric_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "- After analyzing the boxplots of the numerical features, we noticed that there are no extreme values on the left side (lower bound) of the box plots, meaning that the majority of them are in the right side (upper bound) of the box plots.\n",
    "- With that in mind, we decided to go with the rule of thumb in data analysis of choosing the 99th percentile as a threshold for handling outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to treat outliers of only three features *customer_age*, *vendor_count*, and *product_count*, because we believed it was better to leave outliers in the remaining features for better segmentation of customers.<br>\n",
    "We are going to check the difference between the max value and 99th percentile of the features: *customer_age*, *vendor_count*, and *product_count*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable of three features that we are going to treat\n",
    "columns_to_check = ['customer_age', 'vendor_count', 'product_count']\n",
    "\n",
    "percentile_threshold = 0.99\n",
    "# Define 99th percentile as threshould\n",
    "thresholds = {col: df[col].quantile(percentile_threshold) for col in columns_to_check}\n",
    "\n",
    "# Compare the max value and 99th percentile in the features\n",
    "for col in columns_to_check:\n",
    "    max_value = df[col].max()\n",
    "    threshold = thresholds[col]\n",
    "    print(f\"{col}: Max = {max_value}, 99th Percentile = {threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As you can see here, just from looking at the 99th percentile compared to the max value from the features, we can confirm our theory that the last 1th percentile contains a significant amount of outliers.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to convert the outliers of these three features to the 99th percentile of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat outliers of these three features\n",
    "for col in columns_to_check:\n",
    "    upper_limit = thresholds[col]\n",
    "    # If a value in the column is greater than 99th percentile, replace it with 99th percentile\n",
    "    df[col] = np.where(df[col] > upper_limit, upper_limit, df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create box plots to check if the changes were applied correctly\n",
    "for col in columns_to_check:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=df, x=col)\n",
    "    plt.title(f\"Box Plot of {col} After Outlier Handling\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"13\">     \n",
    "\n",
    "# 6. New Feature Creation\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"14\">     \n",
    "\n",
    "## 6.1. days_as_customer\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create new feature *days_as_customer* that provides us how long each customer has used our services over a three-month period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the days the customer has used our service\n",
    "df['days_as_customer'] = df['last_order'] - df['first_order']\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram\n",
    "plt.hist(df['days_as_customer'], bins=20, color='grey', edgecolor='black')\n",
    "plt.title('Histogram of Days as Customer')\n",
    "plt.xlabel('Days as Customer')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Negative_days_counts = df[df['days_as_customer'] < 0]['days_as_customer'].value_counts()\n",
    "print(Negative_days_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After viewing the output of this new feature, we realized that there are 106 customers that had a negative value -22 for the *days_as_customers* which is not acceptable. The reasoning we hypothosized is those 106 customers had *first_order* of NaN and *last_order* less than the median of the *first_order* before treating the outliers. Therefore, to make sure the data is consistant and without errors, we decided to remove them from our dataset since their number is not big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the rows that have negative days in 'days_as_customer'\n",
    "negative_days_rows = df[df['days_as_customer'] < 0]\n",
    "negative_days_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify rows where 'days_as_client' is negative\n",
    "negative_days = df[df['days_as_customer'] < 0].index\n",
    "\n",
    "# Drop rows with inplace=True\n",
    "df.drop(index=negative_days, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the change was applied correctly\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"15\">     \n",
    "\n",
    "## 6.2. money_spent \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create new feature *money_spent* that provides us the total amount spent by each customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all cuisine columns, including the combined CUI_OTHER\n",
    "cuisine_columns = [col for col in df.columns if 'CUI_' in col]  \n",
    "\n",
    "# Creat new feature 'money_spent' \n",
    "df['money_spent'] = df[cuisine_columns].sum(axis=1)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"16\">\n",
    "\n",
    "## 6.3. customer_city\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the previous steps, we found that are there eight regions instead of three cities. Therefore, we decided to group regions 2360, 2440, and 2490 as city A, regions 4660 and 4140 as city B, and regions 8000, 8670, 8370, and 8550 as city C and create a new feature customer_city. This is because regions starting with the same number are considered the same city, since their distribution is the same as that observed in the data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping for the regions with string keys\n",
    "region_to_city = {\n",
    "    '2360': 'City A', '2440': 'City A', '2490': 'City A',\n",
    "    '4660': 'City B', '4140': 'City B',\n",
    "    '8670': 'City C', '8370': 'City C', '8550': 'City C','8000':'City C'\n",
    "}\n",
    "\n",
    "# Create new feature 'customer_city'\n",
    "df['customer_city'] = df['customer_region'].map(region_to_city)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the values of new feature\n",
    "df['customer_city'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"17\">     \n",
    "\n",
    "## 6.4. total_orders\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create new feature *total_orders* that provides us the total number of orders made by each customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of DOW_ columns to sum\n",
    "columns_to_sum = ['DOW_0', 'DOW_1', 'DOW_2', 'DOW_3', 'DOW_4', 'DOW_5', 'DOW_6']\n",
    "\n",
    "# Create new feature 'total_order' \n",
    "df['total_orders'] = df[columns_to_sum].sum(axis=1)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"18\">     \n",
    "\n",
    "## 6.5. average_purchase\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create new feature *average_purchase* that provides us the average purchase amount per order for each customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature 'average_purchase'\n",
    "df['average_purchase'] = df['money_spent'] / df['total_orders']\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"19\">     \n",
    "\n",
    "## 6.6. age_group\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create new feature *age_group* that provides the age group to which each customer belongs.<br>\n",
    "We defined the age categories as follows:<br>\n",
    "- Child: Below 17\n",
    "- Young Adult: 18 - 34\n",
    "- Adult: 35 - 59\n",
    "- Senior: Above 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define age categories\n",
    "bins = [0, 18, 35, 60, float('inf')]  \n",
    "labels = ['Child', 'Young Adult', 'Adult', 'Senior']\n",
    "\n",
    "# Create new feature 'age_group'\n",
    "df['age_group'] = pd.cut(df['customer_age'], bins=bins, labels=labels, right=False)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"20\">     \n",
    "\n",
    "## 6.7. morning_orders, afternoon_orders, evening_orders, night_orders\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create four new features: *morning_orders*, *afternoon_orders*, *evening_orders*, and *night_orders* using *HR_* features that provide us which time of period each customer has ordered.<br>\n",
    "We defined each features (groups) as follows: <br>\n",
    "- morning_columns: HR_6, HR_7, HR_8, HR_9, HR_10, HR_11\n",
    "- afternoon_columns: HR_12, HR_13, HR_14, HR_15, HR_16, HR_17\n",
    "- evening_columns: HR_18, HR_19, HR_20, HR_21, HR_22, HR_23\n",
    "- night_columns: HR_0, HR_1, HR_2, HR_3, HR_4, HR_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define periods of the day\n",
    "morning_columns = ['HR_6', 'HR_7', 'HR_8', 'HR_9', 'HR_10', 'HR_11']\n",
    "afternoon_columns = ['HR_12', 'HR_13', 'HR_14', 'HR_15', 'HR_16', 'HR_17']\n",
    "evening_columns = ['HR_18', 'HR_19', 'HR_20', 'HR_21', 'HR_22', 'HR_23']\n",
    "night_columns = ['HR_0', 'HR_1', 'HR_2', 'HR_3', 'HR_4', 'HR_5']\n",
    "\n",
    "# Create new features 'morning_orders', 'afternoon_orders', 'evening_orders', and 'night_orders'\n",
    "df['morning_orders'] = df[morning_columns].sum(axis=1)\n",
    "df['afternoon_orders'] = df[afternoon_columns].sum(axis=1)\n",
    "df['evening_orders'] = df[evening_columns].sum(axis=1)\n",
    "df['night_orders'] = df[night_columns].sum(axis=1)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"21\">     \n",
    "\n",
    "## 6.8. customer_frequency\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create new feature *customer_frequency* that provides us with a number of times (level) each customer has used our services.<br>\n",
    "We used quantiles to decide each category as follows:<br>\n",
    "- Infrequent: Under 25% (Q1)\n",
    "- Moderate: Between 25% and 75% (Q1 to Q3)\n",
    "- Frequent: Over 75% (Q3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentiles\n",
    "q1 = df['total_orders'].quantile(0.25)\n",
    "q3 = df['total_orders'].quantile(0.75)\n",
    "\n",
    "# Define thresholds based on percentiles\n",
    "df['customer_frequency'] = pd.cut(df['total_orders'], \n",
    "                             bins=[-float('inf'), q1, q3, float('inf')], \n",
    "                             labels=['Infrequent', 'Moderate', 'Frequent'])\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the values in the new feature\n",
    "df['customer_frequency'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Infrequent_counts = df[df['customer_frequency'] == 'Infrequent']['total_orders'].value_counts()\n",
    "print(Infrequent_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Moderate_counts = df[df['customer_frequency'] == 'Moderate']['total_orders'].value_counts()\n",
    "print(Moderate_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_counts = df[df['customer_frequency'] == 'Frequent']['total_orders'].value_counts()\n",
    "print(frequent_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"25th Percentile (Q1): {q1}\")\n",
    "print(f\"75th Percentile (Q3): {q3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"22\">     \n",
    "\n",
    "## 6.9. customer_preference\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create new feature *customer_preference* that provides us the cuisine most ordered by each customer. We then assume that it is the dish that each customer prefers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all cuisine columns explicitly\n",
    "cuisine_columns = [\n",
    "    \"CUI_American\", \"CUI_Asian\", \"CUI_Beverages\", \"CUI_Cafe\", \"CUI_Chicken Dishes\",\n",
    "    \"CUI_Chinese\", \"CUI_Desserts\", \"CUI_Healthy\", \"CUI_Indian\", \"CUI_Italian\",\n",
    "    \"CUI_Japanese\", \"CUI_Noodle Dishes\", \"CUI_OTHER\", \"CUI_Street Food / Snacks\", \"CUI_Thai\"\n",
    "]\n",
    "\n",
    "# Apply idxmax only to the cuisine columns\n",
    "df['customer_preference'] = df[cuisine_columns].apply(lambda row: row.idxmax(), axis=1)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"23\">     \n",
    "\n",
    "## 6.10. order_frequency\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create new feature *order_frequency* that provide us the frequency of orders made by each customer over a 3 month period, by *total_orders* devided by 90 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['order_frequency'] = df['total_orders'] / 90\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the values of the new feature\n",
    "df['order_frequency'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"24\">     \n",
    "\n",
    "## 6.11. weekend_orders\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create new feature *weekend_orders* that provides us the number of the orders that were made in the weekend days, DOW_6 (Saturday) and DOW_0 (Sunday)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'DOW_6' represents Saturday and 'DOW_0' represents Sunday\n",
    "df['weekend_orders'] = df['DOW_6'] + df['DOW_0']\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the values of the new feature\n",
    "df['weekend_orders'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"25\">     \n",
    "\n",
    "## 6.12. week_orders\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create new feature *week_orders* that provides us the number of the orders that were made in the week days, from DOW_1 (Monday) to DOW_5 (Friday)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['week_orders'] = df['DOW_1'] + df['DOW_2'] + df['DOW_3']+ df['DOW_4'] + df['DOW_5']\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the values in the new feature\n",
    "df['week_orders'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"26\">     \n",
    "\n",
    "## 6.13. Display New Features\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable of the new features\n",
    "new_features = [\n",
    "    'days_as_customer', 'money_spent', 'customer_city',\n",
    "    'total_orders','average_purchase','age_group', 'morning_orders',\n",
    "    'afternoon_orders', 'evening_orders', 'night_orders',\n",
    "    'customer_frequency', 'customer_preference','order_frequency',\n",
    "    'weekend_orders', 'week_orders'\n",
    "    ]\n",
    "\n",
    "# Show the first few rows of the new features\n",
    "df[new_features].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"27\">     \n",
    "\n",
    "# 7. Data Pre-Processing 2\n",
    "</a>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"28\">     \n",
    "\n",
    "## 7.1. Checking The Dataset\n",
    "</a>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include = \"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no missing values in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"29\">     \n",
    "\n",
    "## 7.2. Visualization of New Features \n",
    "</a>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"30\">     \n",
    "\n",
    "## 7.3. Treating Outliers of New Features \n",
    "</a>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variable of new features\n",
    "new_features = [\n",
    "    'days_as_customer', 'money_spent', 'customer_city',\n",
    "    'total_orders','average_purchase','age_group', 'morning_orders',\n",
    "    'afternoon_orders', 'evening_orders', 'night_orders',\n",
    "    'customer_frequency', 'customer_preference','order_frequency',\n",
    "    'weekend_orders', 'week_orders'\n",
    "    ]\n",
    "\n",
    "# Set up the grid for plots\n",
    "num_features = len(new_features)\n",
    "cols = 3  \n",
    "rows = (num_features + cols - 1) // cols  \n",
    "\n",
    "# Create count plots for categorical features and box plots for numerical features\n",
    "plt.figure(figsize=(20, 5 * rows))\n",
    "for i, feature in enumerate(new_features, 1):\n",
    "    plt.subplot(rows, cols, i)\n",
    "    \n",
    "    # Check the df type to determine plot type\n",
    "    if df[feature].dtype == 'object' or df[feature].nunique() < 10:  #Categorical features\n",
    "        sns.countplot(y=feature, data=df, hue=feature)\n",
    "        plt.legend().remove() \n",
    "        plt.title(f'Count Plot of {feature}', fontsize=14)\n",
    "        plt.xlabel('Count', fontsize=12)\n",
    "        plt.ylabel(feature, fontsize=12)\n",
    "    else:  # Numerical variables\n",
    "        sns.boxplot(x=df[feature], color='skyblue')  # Horizontal boxplot\n",
    "        plt.title(f'Boxplot of {feature}', fontsize=14)\n",
    "        plt.xlabel('Value', fontsize=12)\n",
    "        plt.ylabel(feature, fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the box plots above, we perceive that there are extreme outliers in the numeric features except *days_as_customer*.<br>\n",
    "Although we have tried multiple technic to deal with those outliers such as 99th percentile as we have treated outliers in the previous steps ,IQR, and also both of them (hybrid), we believe that the manual outlier treatment would work with our new features. Thus, we are going to set the threshold on all of new features, then convert the outliers to the highest value in the range. <br><br>\n",
    "\n",
    "We defined the threshold for outliers for each features as follows:\n",
    "- *money_spent*: 800\n",
    "- *total_orders*: 75\n",
    "- *average_purchase*: 100\n",
    "- *morning_orders*: 45\n",
    "- *afternoon_orders*: 37\n",
    "- *evening_orders*: 30\n",
    "- *night_orders*: 20\n",
    "- *order_frequency*: 0.8\n",
    "- *weekend_orders*: 27.5\n",
    "- *week_orders*: 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with features and their corresponding thresholds\n",
    "feature_thresholds = {\n",
    "    'money_spent': 800,\n",
    "    'total_orders': 75,\n",
    "    'average_purchase': 100,\n",
    "    'morning_orders': 45,\n",
    "    'afternoon_orders': 37,\n",
    "    'evening_orders': 30,\n",
    "    'night_orders': 20,\n",
    "    'order_frequency': 0.8,\n",
    "    'weekend_orders': 27.5,\n",
    "    'week_orders': 50\n",
    "}\n",
    "\n",
    "# Cap values for each feature based on the thresholds\n",
    "for feature, threshold in feature_thresholds.items():\n",
    "    initial_count = df.shape[0]  # Total rows before filtering\n",
    "    df = df[df[feature] < threshold]  # Keep only rows below the threshold\n",
    "    final_count = df.shape[0]  # Total rows after filtering\n",
    "    print(f\"Feature '{feature}': Threshold = {threshold}. Removed {initial_count - final_count} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"31\">     \n",
    "\n",
    "# 8. Feature Engineering\n",
    "</a>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"32\">     \n",
    "\n",
    "## 8.1. Dropping features\n",
    "</a>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided to remove the following features, stating our reasons for doing so.\n",
    "\n",
    "| Feature | Reason to remove |\n",
    "|---------|----------------|\n",
    "| last_promo | It has exaggerated number (16748) of missing values.|\n",
    "| customer_region | We have created new feature *customer_city* instead. |\n",
    "| DOW_0, DOW_1, DOW_2, DOW_3, DOW_4, DOW_5, DOW_6 | We have created new feature *total_orders* instead. |\n",
    "| HR_6, HR_7, HR_8, HR_9, HR_10, HR_11 | We have created new feature *morning_orders* instead. |\n",
    "| HR_12, HR_13, HR_14, HR_15, HR_16, HR_17 | We have created new feature *afternoon_orders* instead. |\n",
    "| HR_18, HR_19, HR_20, HR_21, HR_22, HR_23 | We have created new feature *evening_orders* instead. |\n",
    "| HR_0, HR_1, HR_2, HR_3, HR_4, HR_5 | We have created new feature *night_orders* instead. |\n",
    "| last_order, first_order | We have created new feature *days_as_customer* instead. |\n",
    "| CUI_ | We have created new feature *customer_preference* instead. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify features to drop\n",
    "columns_to_drop = [\n",
    "    'customer_region', 'DOW_0', 'DOW_1', 'DOW_2', 'DOW_3', 'DOW_4', 'DOW_5', 'DOW_6',\n",
    "    'HR_0', 'HR_1', 'HR_2', 'HR_3', 'HR_4', 'HR_5', 'HR_6', 'HR_7', 'HR_8', 'HR_9',\n",
    "    'HR_10', 'HR_11', 'HR_12', 'HR_13', 'HR_14', 'HR_15', 'HR_16', 'HR_17', 'HR_18',\n",
    "    'HR_19', 'HR_20', 'HR_21', 'HR_22', 'HR_23', 'first_order', 'last_order',\n",
    "     'CUI_American', 'CUI_Asian', 'CUI_Beverages',\n",
    "    'CUI_Cafe', 'CUI_Chicken Dishes', 'CUI_Chinese', 'CUI_Desserts', 'CUI_Healthy',\n",
    "    'CUI_Indian', 'CUI_Italian', 'CUI_Japanese', 'CUI_Noodle Dishes', 'CUI_OTHER',\n",
    "    'CUI_Street Food / Snacks', 'CUI_Thai', 'last_promo'\n",
    "]\n",
    "\n",
    "# Drop the features\n",
    "df.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.remove('customer_region', axis=1, inplace=True)\n",
    "df.remove('DOW_0', axis=1, inplace=True)\n",
    "df.remove('DOW_1', axis=1, inplace=True)\n",
    "df.remove('DOW_2', axis=1, inplace=True)\n",
    "df.remove('DOW_3', axis=1, inplace=True)\n",
    "df.remove('DOW_4', axis=1, inplace=True)\n",
    "df.remove('DOW_5', axis=1, inplace=True)\n",
    "df.remove('DOW_6', axis=1, inplace=True)\n",
    "df.remove('HR_0', axis=1, inplace=True)\n",
    "df.remove('HR_1', axis=1, inplace=True)\n",
    "df.remove('HR_2', axis=1, inplace=True)\n",
    "df.remove('HR_3', axis=1, inplace=True)\n",
    "df.remove('HR_4', axis=1, inplace=True)\n",
    "df.remove('HR_5', axis=1, inplace=True)\n",
    "df.remove('HR_6', axis=1, inplace=True)\n",
    "df.remove('HR_7', axis=1, inplace=True)\n",
    "df.remove('HR_8', axis=1, inplace=True)\n",
    "df.remove('HR_9', axis=1, inplace=True)\n",
    "df.remove('HR_10', axis=1, inplace=True)\n",
    "df.remove('HR_11', axis=1, inplace=True)\n",
    "df.remove('HR_12', axis=1, inplace=True)\n",
    "df.remove('HR_13', axis=1, inplace=True)\n",
    "df.remove('HR_14', axis=1, inplace=True)\n",
    "df.remove('HR_15', axis=1, inplace=True)\n",
    "df.remove('HR_16', axis=1, inplace=True)\n",
    "df.remove('HR_17', axis=1, inplace=True)\n",
    "df.remove('HR_18', axis=1, inplace=True)\n",
    "df.remove('HR_19', axis=1, inplace=True)\n",
    "df.remove('HR_20', axis=1, inplace=True)\n",
    "df.remove('HR_21', axis=1, inplace=True)\n",
    "df.remove('HR_22', axis=1, inplace=True)\n",
    "df.remove('HR_23', axis=1, inplace=True)\n",
    "df.remove('first_order', axis=1, inplace=True)\n",
    "df.remove('last_order', axis=1, inplace=True)\n",
    "df.remove('total_orders', axis=1, inplace=True)\n",
    "df.remove('customer_age', axis=1, inplace=True)\n",
    "df.remove('CUI_American', axis=1, inplace=True)\n",
    "df.remove('CUI_Asian', axis=1, inplace=True)\n",
    "df.remove('CUI_Beverages', axis=1, inplace=True)\n",
    "df.remove('CUI_Cafe', axis=1, inplace=True)\n",
    "df.remove('CUI_Chicken Dishes', axis=1, inplace=True)\n",
    "df.remove('CUI_Chinese', axis=1, inplace=True)\n",
    "df.remove('CUI_Desserts', axis=1, inplace=True)\n",
    "df.remove('CUI_Healthy', axis=1, inplace=True)\n",
    "df.remove('CUI_Indian', axis=1, inplace=True)\n",
    "df.remove('CUI_Italian', axis=1, inplace=True)\n",
    "df.remove('CUI_Japanese', axis=1, inplace=True)\n",
    "df.remove('CUI_Noodle Dishes', axis=1, inplace=True)\n",
    "df.remove('CUI_OTHER', axis=1, inplace=True)\n",
    "df.remove('CUI_Street Food / Snacks', axis=1, inplace=True)\n",
    "df.remove('CUI_Thai', axis=1, inplace=True)\n",
    "df.remove('last_promo', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"33\">     \n",
    "\n",
    "## 8.2. Changing Data Types\n",
    "</a>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data types\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided to convert the data types of the following features for data scalling process.\n",
    "- *is_chain*: Boolean to Object\n",
    "- *age_group*: Category to Object\n",
    "- *customer_frequency*: Category to Object\n",
    "- *customer_age*: Float to Integer\n",
    "- *vendor_count*: Float to Integer\n",
    "- *product_count*: Float to Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of features to convert to object\n",
    "object_columns=['is_chain','age_group','customer_frequency']\n",
    "\n",
    "# Change the data type to object\n",
    "for dataset in [df]:\n",
    "    dataset[object_columns] = dataset[object_columns].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of features to convert to integer\n",
    "int_columns = ['customer_age', 'vendor_count', 'product_count']\n",
    "\n",
    "# Change the data type to integer\n",
    "for dataset in [df]:\n",
    "    dataset[int_columns] = dataset[int_columns].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the change was applied correctly\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"34\">     \n",
    "\n",
    "## 8.3. Splitting Metric Features and Non-Metric Features\n",
    "</a>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting feature names into groups\n",
    "metric_features = [\n",
    "    'vendor_count',\n",
    "    'product_count',\n",
    "    'customer_age',\n",
    "    'days_as_customer',\n",
    "    'money_spent',\n",
    "    'total_orders',\n",
    "    'average_purchase',\n",
    "    'morning_orders',\n",
    "    'afternoon_orders',                \n",
    "    'evening_orders',\n",
    "    'night_orders',\n",
    "    'order_frequency',\n",
    "    'weekend_orders',\n",
    "    'week_orders'\n",
    " ]\n",
    "\n",
    "non_metric_features = [\n",
    "    'is_chain',\n",
    "    'payment_method',\n",
    "    'customer_city',\n",
    "    'age_group',\t\n",
    "    'customer_frequency',\n",
    "    'customer_preference'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[metric_features].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[non_metric_features].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"35\">     \n",
    "\n",
    "# 9. Data Scaling (WIP: try different scaler)\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(df[metric_features])\n",
    "\n",
    "# Convert back to a DataFrame with original column names\n",
    "df[metric_features] = scaler.fit_transform(df[metric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the change was applied correctly\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"36\">     \n",
    "\n",
    "# 10. Checking Redundancy and Relevency\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable that gives us the correlation map of the metric features\n",
    "correlation_matrix = df[metric_features].corr()\n",
    "\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap\n",
    "plt.figure(figsize=(10, 8))  \n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.8\n",
    "\n",
    "# Find feature pairs with correlation greater than the threshold\n",
    "high_corr_pairs = correlation_matrix.stack()[correlation_matrix.stack() > threshold]\n",
    "\n",
    "# Extract unique features that are part of these high correlations\n",
    "unique_features = set(high_corr_pairs.index.get_level_values(0)).union(\n",
    "    set(high_corr_pairs.index.get_level_values(1))\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"Highly Correlated Pairs:\")\n",
    "print(high_corr_pairs)\n",
    "print(\"\\nNumber of unique features involved in high correlations:\")\n",
    "print(len(unique_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate pairwise correlations\n",
    "threshold = 0.8\n",
    "high_corr_pairs = correlation_matrix[(correlation_matrix > threshold) & (correlation_matrix < 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_corr_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vendor_count\n",
    "# day_as_customer\n",
    "# money_spent\n",
    "# total_orders\n",
    "# average_purchase\n",
    "# morning_orders\n",
    "# afternoon_orders\n",
    "# evening_orders\n",
    "# weekend_orders\n",
    "\n",
    "redundant_features = [\n",
    "    'product_count',\n",
    "    'order_frequency',\n",
    "    'week_orders'\n",
    "]\n",
    "\n",
    "irrelevent_features = [\n",
    "    'night_orders',\n",
    "    'customer_age'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric_features.remove('product_count')\n",
    "# metric_features.remove('order_frequency')\n",
    "# metric_features.remove('week_orders')\n",
    "# metric_features.remove('night_orders')\n",
    "# metric_features.remove('customer_age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[metric_features].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path and format\n",
    "output_file = 'project_data/preprocessed_dataset.csv'  # You can change the file name and path\n",
    "\n",
    "# Export the dataset to a CSV file\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Dataset exported successfully to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
